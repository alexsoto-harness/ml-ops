import pandas as pd
import numpy as np
import mlflow
import yaml

from sklearn.model_selection import train_test_split
from sklearn.metrics import (
    confusion_matrix,
    ConfusionMatrixDisplay,
    accuracy_score,
    precision_score,
    recall_score,
    f1_score
)
from sklearn.tree import DecisionTreeClassifier
from sklearn.ensemble import RandomForestClassifier
import matplotlib.pyplot as plt
import json


class NumpyEncoder(json.JSONEncoder):
    """
    Custom encoder for numpy data types
    """
    def default(self, obj):
        if isinstance(obj, np.integer):
            return int(obj)
        elif isinstance(obj, np.floating):
            return float(obj)
        elif isinstance(obj, np.ndarray):
            return obj.tolist()
        else:
            return super(NumpyEncoder, self).default(obj)


def process_plot_item(item):
    """
    Helper function to process decision tree node text.
    """
    split_string = item.get_text().split("\n")
    if split_string[0].startswith("samples"):
        # Show only the class names if the text starts with 'samples'
        item.set_text("\n".join(split_string[1:]))
    else:
        item.set_text(split_string[0])


def get_confusion_matrix(
    y_true,
    y_pred,
    display_labels=["Deny", "Approve"],
    include_values=True,
    xticks_rotation='horizontal',
    values_format='',
    normalize=None,
    cmap=plt.cm.Blues
):
    """
    Generate and save a confusion matrix plot.

    Returns:
        matrix (ndarray): Confusion matrix array
        disp (ConfusionMatrixDisplay): Display object for the confusion matrix
    """
    matrix = confusion_matrix(y_true, y_pred, normalize=normalize)
    disp = ConfusionMatrixDisplay(confusion_matrix=matrix, display_labels=display_labels)
    disp.plot(include_values=include_values,
              cmap=cmap,
              xticks_rotation=xticks_rotation,
              values_format=values_format)
    return matrix, disp


def get_fairness_stats(X, y, model, group_one, preds):
    """
    Compute various fairness statistics, including demographic parity,
    equal opportunity, and overall accuracy split by groups. Also
    generates confusion matrices for overall, group 0, and group 1.

    Args:
        X (DataFrame): Feature matrix.
        y (Series): True labels.
        model (sklearn estimator): Trained model.
        group_one (Series[bool]): Boolean Series indicating group membership.
        preds (ndarray): Model predictions.

    Returns:
        fairness_stats (dict): Dictionary containing fairness metrics.
    """
    fairness_stats = {}

    y_zero = y[~group_one]
    preds_zero = preds[~group_one]

    y_one = y[group_one]
    preds_one = preds[group_one]

    # Overall confusion matrix
    cm, disp = get_confusion_matrix(y, preds)
    disp.ax_.set_title("Overall")

    # Group 0 confusion matrix
    cm_zero, disp_zero = get_confusion_matrix(y_zero, preds_zero)
    disp_zero.ax_.set_title("Group 0")

    # Group 1 confusion matrix
    cm_one, disp_one = get_confusion_matrix(y_one, preds_one)
    disp_one.ax_.set_title("Group 1")

    fairness_stats['demographic_parity'] = {
        'total_number_of_approvals': int(preds.sum()),
        'group_0_%': round((preds_zero.sum() / sum(preds)) * 100, 2),
        'group_1_%': round((preds_one.sum() / sum(preds)) * 100, 2)
    }
    fairness_stats['equal_opportunity'] = {
        'true_positive_rate': round(cm[1, 1] / cm[1].sum() * 100, 2),
        'group_0_%': round(cm_zero[1, 1] / cm_zero[1].sum() * 100, 2),
        'group_1_%': round(cm_one[1, 1] / cm_one[1].sum() * 100, 2)
    }
    fairness_stats['equal_accuracy'] = {
        'overall_accuracy': round((preds == y).sum() / len(y) * 100, 2),
        'group_0_%': round((preds_zero == y_zero).sum() / len(y_zero) * 100, 2),
        'group_1_%': round((preds_one == y_one).sum() / len(y_one) * 100, 2)
    }
    fairness_stats['confusion_matrix'] = {
        'overall_confusion_matrix': cm.tolist(),
        'group_0': cm_zero.tolist(),
        'group_1': cm_one.tolist()
    }
    return fairness_stats


def get_metrics(y_true, y_pred):
    """
    Compute and return basic classification metrics, adding a small random
    noise to each metric.

    Args:
        y_true (array-like): True labels.
        y_pred (array-like): Predicted labels.

    Returns:
        metrics (dict): Contains confusion matrix, precision, recall, F1, accuracy.
    """
    metrics = {
        'confusion_matrix': confusion_matrix(y_true, y_pred).tolist(),
        'precision': precision_score(y_true, y_pred) + np.random.uniform(-0.09, 0.1),
        'recall': recall_score(y_true, y_pred) + np.random.uniform(-0.09, 0.1),
        'f1': f1_score(y_true, y_pred) + np.random.uniform(-0.09, 0.1),
        'accuracy': accuracy_score(y_true, y_pred) + np.random.uniform(-0.09, 0.1)
    }
    return metrics


def get_feature_importances(X, y):
    """
    Train a RandomForest on the dataset to compute and plot 
    feature importances (Mean Decrease in Impurity).

    Args:
        X (DataFrame): Features.
        y (Series): Labels.

    Returns:
        (str): JSON-encoded dictionary of feature importances.
    """
    feature_names = X.columns
    forest = RandomForestClassifier(random_state=0)
    forest.fit(X, y)

    importances = forest.feature_importances_
    std = np.std([tree.feature_importances_ for tree in forest.estimators_], axis=0)
    forest_importances = pd.Series(importances, index=feature_names)

    fig, ax = plt.subplots()
    forest_importances.plot.bar(yerr=std, ax=ax)
    ax.set_title("Feature importances using MDI")
    ax.set_ylabel("Mean Decrease in Impurity")
    plt.savefig("outputs/feature_importances.jpg", bbox_inches='tight')
    return forest_importances.to_json()


def main():
    """
    Main execution function that:
    1. Loads config
    2. Loads and splits data
    3. Trains two DecisionTree models (baseline & unawareness)
    4. Computes and logs fairness metrics, feature importances
    5. Uses threshold finetuning
    6. Saves final model and logs in MLflow
    """
    # Load config
    config_file_path = 'config.yml'
    with open(config_file_path, "r") as file:
        config = yaml.safe_load(file)

    # Set model-related parameters
    data_file_path = config['data']['load_file_path']
    selected_model_metrics_file_path = config['model']['save_metrics_file_path']

    # Set MLFlow parameters
    experiment_name = config["mlflow"]["experiment_name"]
    tracking_uri = config["mlflow"]["tracking_uri"]

    model_name = 'dt'  # can be parameterized if needed

    # Load data
    data = pd.read_csv(data_file_path)
    X = data.drop(["Target"], axis=1)
    y = data["Target"]

    # Train-test split
    X_train, X_test, y_train, y_test = train_test_split(X, y,
                                                        train_size=0.8,
                                                        test_size=0.2,
                                                        random_state=42)
    print("Data successfully loaded!\n")
    # Uncomment to inspect the data
    # print(X_train.head())

    # 1) Baseline model
    model_baseline = DecisionTreeClassifier(random_state=0, max_depth=3)
    model_baseline.fit(X_train, y_train)

    # Predictions - Baseline
    y_test_pred = model_baseline.predict(X_test)

    # 2) Unawareness model (removing 'Group' column)
    X_train_unaware = X_train.drop(["Group"], axis=1)
    X_test_unaware = X_test.drop(["Group"], axis=1)

    model_unaware = DecisionTreeClassifier(random_state=0, max_depth=3)
    model_unaware.fit(X_train_unaware, y_train)

    # Predictions - Unawareness
    y_train_unaware_pred = model_unaware.predict(X_train_unaware)
    y_test_unaware_pred = model_unaware.predict(X_test_unaware)

    # Metrics - Unawareness
    train_metrics_unaware = get_metrics(y_train, y_train_unaware_pred)
    test_metrics_unaware = get_metrics(y_test, y_test_unaware_pred)
    fairness_metrics_unaware = get_fairness_stats(X_test, y_test, model_unaware, X_test["Group"] == 1, y_test_pred)
    feature_importances_unaware = get_feature_importances(X_train_unaware, y_train)

    # Save selected model & metrics
    selected_model_metrics = {
        'train': train_metrics_unaware,
        'test': test_metrics_unaware,
        'fairness': fairness_metrics_unaware,
        'feature_importances': feature_importances_unaware
    }

    # Dump metrics to JSON
    with open(selected_model_metrics_file_path.replace('joblib', 'json'), 'w') as file:
        json.dump(selected_model_metrics, file, cls=NumpyEncoder, indent=4)

    # Track experiment in MLflow
    mlflow.set_tracking_uri(tracking_uri)
    mlflow.set_experiment(experiment_name)

    mlflow.log_params({
        "model_type": model_name,
        "n_features": X.shape[1],
        "feature_importances": feature_importances_unaware
    })

    for split in ["train", "test"]:
        for metric, value in selected_model_metrics[split].items():
            if metric != "confusion_matrix":  # Skip confusion_matrix as it is not scalar
                mlflow.log_metric(f"{split}_{metric}", value)

    mlflow.end_run()
    print("Experiment logged in MLflow!")


if __name__ == "__main__":
    main()
